{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "KotOnm504AY8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "4f8d2d57-74c7-440d-b9b2-d894ac21d80e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "  0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "* 2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: openjdk version \"1.8.0_432\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_432-8u432-ga~us1-0ubuntu2~22.04-ga)\n",
            "OpenJDK 64-Bit Server VM (build 25.432-bga, mixed mode)\n"
          ]
        }
      ],
      "source": [
        "# Actualizamos los repositorios\n",
        "!apt-get update -qq\n",
        "\n",
        "# Instalamos Spark para Python\n",
        "!pip install pyspark\n",
        "\n",
        "# Instalamos Java 8 (versión ligera sin GUI)\n",
        "!apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Configuramos JAVA_HOME\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "\n",
        "# Seleccionamos Java 8 (si es necesario)\n",
        "!echo 2 | update-alternatives --config java\n",
        "\n",
        "# Validamos instalación de Java\n",
        "!java -version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "\n",
        "# Crear siempre una nueva SparkSession\n",
        "# Detenemos cualquier SparkSession o SparkContext existente para asegurarnos\n",
        "# que el nuevo contexto esté limpio.\n",
        "try:\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "finally:\n",
        "  spark = SparkSession.builder.appName(\"CABD-TP2\").getOrCreate()\n",
        "\n",
        "# Definir rutas\n",
        "root_path = '/content/'\n",
        "input_path = root_path + \"/input/jugadores.txt\"\n",
        "output_path = root_path + \"/output/output1.txt\"\n",
        "# Crear el directorio de salida si no existe\n",
        "if not os.path.exists(os.path.dirname(output_path)):\n",
        "    os.makedirs(os.path.dirname(output_path))\n",
        "# Eliminar el archivo de salida si ya existe\n",
        "if os.path.exists(output_path):\n",
        "    os.remove(output_path)\n",
        "\n",
        "# Estructura del archivo:\n",
        "# ID_Jugador, ID_Misión, ID_Héroe_1, ID_Héroe_2, ID_Héroe_3, ID_Héroe_4,\n",
        "# ID_Héroe_5, Puntaje obtenido, Tiempo de la misión en segundos\n",
        "\n",
        "# Leer, formatear datos y transformar a RDD KeyPair\n",
        "jugadores = spark.sparkContext.textFile(input_path).map(lambda line: line.split(\"\\t\"))\n",
        "misiones = jugadores.map(lambda fields: (int(fields[1]), 1))\n",
        "\n",
        "\n",
        "mision_mas_jugada = misiones.reduceByKey(lambda row1, row2: row1 + row2) \\\n",
        "                    .takeOrdered(1, key=lambda x: -x[1])[0]\n",
        "\n",
        "# Mostrar y guardar resultado\n",
        "print(f\"Misión más jugada: la {mision_mas_jugada[0]}, con {mision_mas_jugada[1]} veces en total.\")\n",
        "with open(output_path, \"w\") as file:\n",
        "    file.write(f\"Misión más jugada: la {mision_mas_jugada[0]}, con {mision_mas_jugada[1]} veces en total.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx6L-TLgW-Aq",
        "outputId": "61b05593-0c70-4c6d-b9ce-c9af48dfd248"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Misión más jugada: la 1, con 22 veces en total.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, Row, Window, functions as F\n",
        "import os\n",
        "\n",
        "# Crear siempre una nueva SparkSession\n",
        "# Detenemos cualquier SparkSession o SparkContext existente para asegurarnos\n",
        "# que el nuevo contexto esté limpio.\n",
        "try:\n",
        "    spark = SparkSession.builder.getOrCreate()\n",
        "    spark.stop()\n",
        "except:\n",
        "    pass\n",
        "finally:\n",
        "  spark = SparkSession.builder.appName(\"CABD-TP2\").getOrCreate()\n",
        "\n",
        "# Definir rutas\n",
        "root_path = '/content'\n",
        "input_path = root_path + \"/input/jugadores.txt\"\n",
        "output_dir = root_path + \"/output\"\n",
        "final_output_path = output_dir + \"/output2_final.txt\"\n",
        "# Crear el directorio de salida si no existe\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "# Eliminar el archivo final si ya existe\n",
        "if os.path.exists(final_output_path):\n",
        "    os.remove(final_output_path)\n",
        "\n",
        "# Leer archivo como RDD\n",
        "jugadores_rdd = spark.sparkContext.textFile(input_path)\n",
        "\n",
        "# Aplanar los héroes por misión\n",
        "misiones_y_heroes_rdd = jugadores_rdd.map(lambda line: line.split(\"\\t\")) \\\n",
        "    .flatMap(lambda fields: [(int(fields[1]), int(fields[i])) for i in range(2, 7)])\n",
        "\n",
        "# Convertir a DataFrame\n",
        "misiones_y_heroes_df = spark.createDataFrame(misiones_y_heroes_rdd, [\"ID_Mision\", \"ID_Heroe\"])\n",
        "\n",
        "# Contar apariciones de héroes por misión\n",
        "conteos = misiones_y_heroes_df.groupBy(\"ID_Mision\", \"ID_Heroe\").count()\n",
        "\n",
        "# Usar ventana para encontrar el máximo conteo por misión\n",
        "windowPorMision = Window.partitionBy(\"ID_Mision\")\n",
        "max_conteo = conteos.withColumn(\"max_conteo\", F.max(\"count\").over(windowPorMision)) \\\n",
        "                    .filter(F.col(\"count\") == F.col(\"max_conteo\")) \\\n",
        "                    .drop(\"max_conteo\") \\\n",
        "                    .orderBy(\"ID_Mision\", \"ID_Heroe\")\n",
        "\n",
        "# Agrupar héroes empatados por misión\n",
        "resultado = max_conteo.groupBy(\"ID_Mision\", \"count\") \\\n",
        "                      .agg(F.collect_list(\"ID_Heroe\").alias(\"Heroes\")) \\\n",
        "                      .orderBy(\"ID_Mision\")\n",
        "\n",
        "# Formatear el resultado por fila\n",
        "def formatear_resultado(row):\n",
        "    mision = row[\"ID_Mision\"]\n",
        "    heroes = row[\"Heroes\"]\n",
        "    count = row[\"count\"]\n",
        "\n",
        "    if len(heroes) == 1:\n",
        "        heroes_str = f\"héroe {heroes[0]}\"\n",
        "    else:\n",
        "        heroes_str = \"héroes \" + \" y \".join(map(str, heroes))\n",
        "\n",
        "    return f\"Misión {mision} ==> {heroes_str} ({count} veces)\\n\"\n",
        "\n",
        "# Guardar distribuidamente\n",
        "def guardar_en_archivo(partition):\n",
        "    # Modo append para no sobreescribir lo que otras particiones escriban\n",
        "    with open(final_output_path, \"a\") as file:\n",
        "        for row in partition:\n",
        "            file.write(formatear_resultado(row))\n",
        "\n",
        "# Pasar a RDD para usar foreachPartition (ejecución del DAG) y guardar distribuidamente\n",
        "resultado.rdd.foreachPartition(guardar_en_archivo)\n",
        "\n",
        "print(f\"Consulta 2 guardada en: {final_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djymOZBS3CKD",
        "outputId": "c6e19c08-485e-4f62-870c-42b0455c8341"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta 2 guardada en: /content/output/output2_final.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "import os\n",
        "\n",
        "# Garantizar una SparkSession limpia\n",
        "def iniciar_spark(app_name):\n",
        "    try:\n",
        "        # Detener cualquier sesión previa\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        spark.stop()\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo detener la sesión previa: {e}\")\n",
        "    finally:\n",
        "        # Crear una nueva sesión\n",
        "        return SparkSession.builder.appName(app_name).getOrCreate()\n",
        "\n",
        "# Crear la SparkSession\n",
        "spark = iniciar_spark(\"CABD-TP3\")\n",
        "\n",
        "# Parámetro H: cantidad mínima de héroes distintos\n",
        "H = 20\n",
        "\n",
        "# Definir rutas\n",
        "root_path = '/content'\n",
        "input_path = root_path + \"/input/jugadores.txt\"\n",
        "output_dir = root_path + \"/output\"\n",
        "final_output_path = output_dir + \"/output3_final.txt\"\n",
        "# Crear el directorio de salida si no existe\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "# Eliminar el archivo final si ya existe\n",
        "if os.path.exists(final_output_path):\n",
        "    os.remove(final_output_path)\n",
        "\n",
        "# Leer el archivo como DataFrame\n",
        "jugadores_df = spark.read.option(\"delimiter\", \"\\t\").csv(input_path, inferSchema=True) \\\n",
        "                         .toDF(\"ID_Jugador\", \"ID_Mision\", \"ID_Heroe_1\", \"ID_Heroe_2\",\n",
        "                               \"ID_Heroe_3\", \"ID_Heroe_4\", \"ID_Heroe_5\", \"Puntaje\", \"Tiempo\")\n",
        "\n",
        "# Aplanar los héroes por jugador, seleccionando las columnas de interés\n",
        "jugador_heroes_df = jugadores_df.select(\n",
        "    \"ID_Jugador\", F.explode(F.array(\"ID_Heroe_1\", \"ID_Heroe_2\", \"ID_Heroe_3\",\n",
        "                                    \"ID_Heroe_4\", \"ID_Heroe_5\")).alias(\"ID_Heroe\")\n",
        ")\n",
        "\n",
        "# Contar héroes distintos por jugador y filtrarlos por parámetro H\n",
        "jugadores_filtrados = jugador_heroes_df.groupBy(\"ID_Jugador\") \\\n",
        "                                      .agg(F.countDistinct(\"ID_Heroe\") \\\n",
        "                                      .alias(\"Cantidad_Heroes\")) \\\n",
        "                                      .filter(F.col(\"Cantidad_Heroes\") > H)\n",
        "\n",
        "# Formatear y guardar el resultado\n",
        "def formatear_resultado(partition):\n",
        "    is_first_partition = False  # Bandera para escribir el encabezado\n",
        "    if not os.path.exists(final_output_path):\n",
        "        is_first_partition = True\n",
        "    with open(final_output_path, \"a\") as file:\n",
        "        if is_first_partition:\n",
        "            file.write(f\"Con H={H}\\n\")\n",
        "        for row in partition:\n",
        "            jugador = row[\"ID_Jugador\"]\n",
        "            cant_heroes = row[\"Cantidad_Heroes\"]\n",
        "            file.write(f\"Jugador {jugador} con {cant_heroes} héroes distintos\\n\")\n",
        "\n",
        "# Formatear y guardar en un archivo de texto distribuidamente (ejecución del DAG)\n",
        "jugadores_filtrados.rdd.foreachPartition(formatear_resultado)\n",
        "\n",
        "print(f\"Consulta 3 guardada en: {final_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scaHz2g0n_8j",
        "outputId": "c16e2c5d-ed19-4719-f28a-6947564feb6b"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta 3 guardada en: /content/output/output3_final.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "import os\n",
        "\n",
        "# Garantizar una SparkSession limpia\n",
        "def iniciar_spark(app_name):\n",
        "    try:\n",
        "        # Detener cualquier sesión previa\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        spark.stop()\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo detener la sesión previa: {e}\")\n",
        "    finally:\n",
        "        # Crear una nueva sesión\n",
        "        return SparkSession.builder.appName(app_name).getOrCreate()\n",
        "\n",
        "# Crear la SparkSession\n",
        "spark = iniciar_spark(\"CABD-TP3\")\n",
        "\n",
        "# Parámetros de la consulta\n",
        "N = 1  # Cantidad mínima de participaciones\n",
        "P = 5000  # Puntaje mínimo total\n",
        "\n",
        "# Definir rutas\n",
        "root_path = '/content'\n",
        "input_path = root_path + \"/input/jugadores.txt\"\n",
        "output_dir = root_path + \"/output\"\n",
        "final_output_path = output_dir + \"/output4_final.txt\"\n",
        "# Crear el directorio de salida si no existe\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "# Eliminar el archivo final si ya existe\n",
        "if os.path.exists(final_output_path):\n",
        "    os.remove(final_output_path)\n",
        "\n",
        "# Leer el archivo como DataFrame\n",
        "jugadores_df = spark.read.option(\"delimiter\", \"\\t\").csv(input_path, inferSchema=True) \\\n",
        "                         .toDF(\"ID_Jugador\", \"ID_Mision\", \"ID_Heroe_1\", \"ID_Heroe_2\",\n",
        "                               \"ID_Heroe_3\", \"ID_Heroe_4\", \"ID_Heroe_5\", \"Puntaje\", \"Tiempo\")\n",
        "\n",
        "conteo_por_misiones = jugadores_df.select(\"ID_Mision\", \"ID_Jugador\") \\\n",
        "                              .groupBy(\"ID_Mision\") \\\n",
        "                                .agg(F.countDistinct(\"ID_Jugador\") \\\n",
        "                                     .alias(\"Jugadores_Distintos\"))\n",
        "\n",
        "# Cachear para prevenir ejecutarse el mismo DAG más de una vez\n",
        "conteo_por_misiones.cache()\n",
        "\n",
        "maximo_conteo_por_misiones = conteo_por_misiones.agg(F.max(\"Jugadores_Distintos\") \\\n",
        "                                   .alias(\"max_distintos\")).first()[\"max_distintos\"]\n",
        "\n",
        "mision_mas_jugada = conteo_por_misiones.filter(\n",
        "                                        F.col(\"Jugadores_Distintos\")\n",
        "                                        == maximo_conteo_por_misiones) \\\n",
        "                                        .agg(F.min(\"ID_Mision\").alias(\"ID_Mision\")) \\\n",
        "                                        .first()[\"ID_Mision\"]\n",
        "\n",
        "# Jugadores que participaron en la misión más jugada\n",
        "jugadores_en_mision_mas_jugada = jugadores_df.filter(\n",
        "                              F.col(\"ID_Mision\")\n",
        "                              == mision_mas_jugada)\n",
        "\n",
        "# Al menos N veces con puntaje > P\n",
        "cumplen = jugadores_en_mision_mas_jugada \\\n",
        "                .groupBy(\"ID_Jugador\") \\\n",
        "                .agg(\n",
        "                F.sum(\"Puntaje\").alias(\"Puntaje_Total\"), \\\n",
        "                F.count(\"ID_Jugador\").alias(\"Cantidad_Participaciones\")\n",
        "                ) \\\n",
        "                .filter((F.col(\"Cantidad_Participaciones\") > N)  \\\n",
        "                & (F.col(\"Puntaje_Total\") > P))\n",
        "\n",
        "# Tiempo total misión más jugada\n",
        "tiempo_mision_mas_jugada = jugadores_en_mision_mas_jugada \\\n",
        "                           .groupBy(\"ID_Mision\") \\\n",
        "                           .agg(\n",
        "                               F.sum(\"Tiempo\").alias(\"Tiempo_Total\")\n",
        "                           ).first()[\"Tiempo_Total\"]\n",
        "\n",
        "# Guardar encabezado y resultados\n",
        "def guardar_resultados(partition):\n",
        "    is_first_partition = not os.path.exists(final_output_path)\n",
        "    with open(final_output_path, \"a\") as file:\n",
        "        if is_first_partition:\n",
        "            file.write(f\"Con N={N} y P={P}\\n\")\n",
        "            file.write(f\"La misión más jugada es la {mision_mas_jugada} con {maximo_conteo_por_misiones} jugadores distintos y {tiempo_mision_mas_jugada} segundos de juego.\\n\")\n",
        "        for row in partition:\n",
        "            file.write(f\"Jugador {row['ID_Jugador']}, participó {row['Cantidad_Participaciones']} veces con {row['Puntaje_Total']} en total\\n\")\n",
        "\n",
        "# Guardar resultados distribuidamente (se ejecuta lo necesario del DAG)\n",
        "cumplen.rdd.foreachPartition(guardar_resultados)\n",
        "\n",
        "print(f\"Consulta 4 guardada en: {final_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdQOu9m1eYJN",
        "outputId": "ac5d21b1-0864-45b8-d346-5f5cd44b8e81"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta 4 guardada en: /content/output/output4_final.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession, functions as F\n",
        "import os\n",
        "\n",
        "# Garantizar una SparkSession limpia\n",
        "def iniciar_spark(app_name):\n",
        "    try:\n",
        "        # Detener cualquier sesión previa\n",
        "        spark = SparkSession.builder.getOrCreate()\n",
        "        spark.stop()\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo detener la sesión previa: {e}\")\n",
        "    finally:\n",
        "        # Crear una nueva sesión\n",
        "        return SparkSession.builder.appName(app_name).getOrCreate()\n",
        "\n",
        "# Crear la SparkSession\n",
        "spark = iniciar_spark(\"CABD-TP3\")\n",
        "\n",
        "# Parámetros de la consulta\n",
        "N = 1  # Cantidad mínima de participaciones\n",
        "P = 5000  # Puntaje mínimo total\n",
        "\n",
        "# Definir rutas\n",
        "root_path = '/content'\n",
        "input_path = root_path + \"/input/jugadores.txt\"\n",
        "output_dir = root_path + \"/output\"\n",
        "final_output_path = output_dir + \"/output5_final.txt\"\n",
        "# Crear el directorio de salida si no existe\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "# Eliminar el archivo final si ya existe\n",
        "if os.path.exists(final_output_path):\n",
        "    os.remove(final_output_path)\n",
        "\n",
        "# Leer el archivo como DataFrame\n",
        "jugadores_df = spark.read.option(\"delimiter\", \"\\t\").csv(input_path, inferSchema=True) \\\n",
        "                         .toDF(\"ID_Jugador\", \"ID_Mision\", \"ID_Heroe_1\", \"ID_Heroe_2\",\n",
        "                               \"ID_Heroe_3\", \"ID_Heroe_4\", \"ID_Heroe_5\", \"Puntaje\", \"Tiempo\")\n",
        "\n",
        "# Aplanar los héroes por jugador, seleccionando las columnas de interés\n",
        "heroes_jugador_df = jugadores_df.select(\n",
        "    \"ID_Jugador\", F.explode(F.array(\"ID_Heroe_1\", \"ID_Heroe_2\", \"ID_Heroe_3\",\n",
        "                                    \"ID_Heroe_4\", \"ID_Heroe_5\")).alias(\"ID_Heroe\")\n",
        ")\n",
        "\n",
        "# Contar jugadores distintos por héroe\n",
        "jugadores_distintos = heroes_jugador_df.groupBy(\"ID_Heroe\") \\\n",
        "                                      .agg(F.countDistinct(\"ID_Jugador\") \\\n",
        "                                      .alias(\"Cantidad_Jugadores\"))\n",
        "\n",
        "\n",
        "# Aplanar los héroes por misión, seleccionando las columnas de interés\n",
        "heroes_mision_df = jugadores_df.select(\n",
        "    \"ID_Mision\", F.explode(F.array(\"ID_Heroe_1\", \"ID_Heroe_2\", \"ID_Heroe_3\",\n",
        "                                    \"ID_Heroe_4\", \"ID_Heroe_5\")).alias(\"ID_Heroe\")\n",
        ")\n",
        "\n",
        "# Contar misiones distintas por héroe\n",
        "misiones_distintas = heroes_mision_df.groupBy(\"ID_Heroe\") \\\n",
        "                                      .agg(F.countDistinct(\"ID_Mision\") \\\n",
        "                                      .alias(\"Cantidad_Misiones\"))\n",
        "\n",
        "indice_p_df = jugadores_distintos.join(misiones_distintas, \"ID_Heroe\") \\\n",
        "              .withColumn(\"Indice_P\", F.least(F.col(\"Cantidad_Jugadores\"), F.col(\"Cantidad_Misiones\"))) \\\n",
        "              .orderBy(\"ID_Heroe\").drop(\"Cantidad_Jugadores\", \"Cantidad_Misiones\")\n",
        "\n",
        "# Formatear el resultado por fila\n",
        "def formatear_resultado(row):\n",
        "    heroe = row[\"ID_Heroe\"]\n",
        "    indice_p = row[\"Indice_P\"]\n",
        "    return f\"Héroe {heroe}: Índice P = {indice_p}\\n\"\n",
        "\n",
        "# Guardar distribuidamente\n",
        "def guardar_en_archivo(partition):\n",
        "    # Modo append para no sobreescribir lo que otras particiones escriban\n",
        "    with open(final_output_path, \"a\") as file:\n",
        "        for row in partition:\n",
        "            file.write(formatear_resultado(row))\n",
        "\n",
        "# Usar foreachPartition (ejecución del DAG) y guardar distribuidamente\n",
        "indice_p_df.rdd.foreachPartition(guardar_en_archivo)\n",
        "\n",
        "print(f\"Consulta 5 guardada en: {final_output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdk8tnevPgFn",
        "outputId": "97c8c535-9b8b-4246-ec61-16e078a00ba8"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Consulta 5 guardada en: /content/output/output5_final.txt\n"
          ]
        }
      ]
    }
  ]
}
